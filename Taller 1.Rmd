---
title: "Taller 1 analisis avanzado de datos"
author: "Hector Rojas"
date: "2023-03-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(tidyverse)

# Gráficos y tratamiento de datos
# ==============================================================================
#devtools::install_github("boxuancui/DataExplorer")
library(scales)
library(corrplot)

# Modelado
# ==============================================================================
install.packages("pls")
install.packages("corrr")
install.packages("skimr")
install.packages("DataExplorer")
install.packages("faraway")
install.packages("ggplot")
install.packages("ggplot2")

```

```{r}
data <- read.delim2(file="taller1.txt", header=TRUE, sep = ",", dec = ".")
data
```

```{r}
modelData <- data.frame(data[,2:5001])
lmmodel <- lm(data$y ~ .,modelData)
summary(lmmodel)
```

```{r}
print(head(modelData))
```

```{r}
cor(data)
```

```{r}
install.packages("glmnet")
```

Dado el analisis anterior podemos evidenciar que para este modelo existe perfecta multicolinealidad entre las variables

Punto 2:

```{r}
Y1 <- data$y
x <- modelData
```

Separando aleatoriamente y guardando la semilla

```{r}
set.seed(123)
train <- sample(nrow(data), nrow(data)*0.8)
x_train <- x[train, ]
y_train <- Y1[train]
x_test <- x[-train, ]
y_test <- Y1[-train]
```

```{r}
x_train <- scale(x_train)
x_test <- scale(x_test)
```

Defniniendo los modelos Ridge y Lasso

```{r}
ridge <- glmnet::glmnet(x_train, y_train, alpha = 0 )
lasso <- glmnet::glmnet(x_train, y_train, alpha = 1)
```

seleccionando el valor de lambda que minimiza el ECM mediante validación cruzada

```{r}
ridge_cv <- glmnet::cv.glmnet(x_train, y_train, alpha = 0)
lasso_cv <- glmnet::cv.glmnet(x_train, y_train, alpha = 1)
```

gráfico del Mean Square Error obtenido por validación cruzada para Lasso

```{r}
plot(lasso_cv)
```

```{r}
paste("Mejor valor de lambda encontrado:", lasso_cv$lambda.min)
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", lasso_cv$lambda.1se)
```

gráfico del Mean Square Error obtenido por validación cruzada para Ridge

```{r}
plot(ridge_cv)
```

```{r}
paste("Mejor valor de lambda encontrado:", ridge_cv$lambda.min)
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", ridge_cv$lambda.1se)
```

```{r}

#codigo temporal pendiente revisarlo y quitarlo de ser necesario
ridge_pred <- predict(ridge_cv, newx = x_test, s = ridge_cv$lambda.min)
lasso_pred <- predict(lasso_cv, newx = x_test, s = lasso_cv$lambda.min)
ridge_ecm <- mean((ridge_pred - y_test)^2)
lasso_ecm <- mean((lasso_pred - y_test)^2)
```

ECM de los modelos Ridge y Lasso

```{r}
ridge_ecm
lasso_ecm
```

Evolución de los coeficientes en función de lambda

```{r}
df_coeficientes_ridge <- coef(ridge) %>%
                   as.matrix() %>%
                   as_tibble(rownames = "predictor") %>%
                   rename(coeficiente_ridge = s0)

df_coeficientes_ridge %>%
  filter(predictor != "(Intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente_ridge)) +
  geom_col() +
  labs(title = "Coeficientes del modelo Ridge") +
  theme_bw() +
  theme(axis.text.x = element_text(size = 6, angle = 45))
```

Evolución de los coeficientes en función de lambda

```{r}
# Evolución de los coeficientes en función de lambda
# ==============================================================================
regularizacion <- lasso$beta %>% 
                  as.matrix() %>%
                  t() %>% 
                  as_tibble() %>%
                  mutate(lambda = lasso$lambda)

regularizacion <- regularizacion %>%
                   pivot_longer(
                     cols = !lambda, 
                     names_to = "predictor",
                     values_to = "coeficientes_lasso"
                   )

regularizacion %>%
  ggplot(aes(x = lambda, y = coeficientes_lasso, color = predictor)) +
  geom_line() +
  scale_x_log10(
    breaks = trans_breaks("log10", function(x) 10^x),
    labels = trans_format("log10", math_format(10^.x))
  ) +
  labs(title = "Coeficientes del modelo Lasso en función de la regularización") +
  theme_bw() +
  theme(legend.position = "none")
```

entrenamos nuevamente los modelos con los mayores valores de lambda encontrados

```{r}
ridge2 <- glmnet::glmnet(x_train, y_train, lambda = ridge_cv$lambda.1se, alpha = 0 )
lasso2 <- glmnet::glmnet(x_train, y_train, lambda = lasso_cv$lambda.1se,  alpha = 1)
```

```{r}
# Predicciones de entrenamiento
# ==============================================================================
predicciones_train <- predict(ridge2, newx = x_train)

# MSE de entrenamiento
# ==============================================================================
training_mse <- mean((predicciones_train - y_train)^2)
paste("Error (mse) de entrenamiento:", training_mse)
```

```{r}
# Predicciones de test
# ==============================================================================
predicciones_test <- predict(ridge2, newx = x_test)

# MSE de test
# ==============================================================================
test_mse_ridge <- mean((predicciones_test - y_test)^2)
paste("Error (mse) de test:", test_mse_ridge)
```

```{r}
# Predicciones de entrenamiento
# ==============================================================================
predicciones_train <- predict(lasso2, newx = x_train)

# MSE de entrenamiento
# ==============================================================================
training_mse <- mean((predicciones_train - y_train)^2)
paste("Error (mse) de entrenamiento:", training_mse)
```

```{r}
# Predicciones de test
# ==============================================================================
predicciones_test <- predict(lasso2, newx = x_test)

# MSE de test
# ==============================================================================
test_mse_lasso <- mean((predicciones_test - y_test)^2)
paste("Error (mse) de test:", test_mse_lasso)
```

```{r}
df_coeficientes_ridge2 <- coef(ridge2) %>%
                   as.matrix() %>%
                   as_tibble(rownames = "predictor") %>%
                   rename(coeficiente_ridge2 = s0)

df_coeficientes_ridge2 %>%
  filter(predictor != "(Intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente_ridge2)) +
  geom_col() +
  labs(title = "Coeficientes del modelo Ridge") +
  theme_bw() +
  theme(axis.text.x = element_text(size = 6, angle = 45))
```

```{r}
# Evolución de los coeficientes en función de lambda
# ==============================================================================
regularizacion2 <- lasso2$beta %>% 
                  as.matrix() %>%
                  t() %>% 
                  as_tibble() %>%
                  mutate(lambda = lasso2$lambda)

regularizacion2 <- regularizacion2 %>%
                   pivot_longer(
                     cols = !lambda, 
                     names_to = "predictor",
                     values_to = "coeficientes_lasso2"
                   )

regularizacion2 %>%
  ggplot(aes(x = lambda, y = coeficientes_lasso2, color = predictor)) +
  geom_line() +
  scale_x_log10(
    breaks = trans_breaks("log10", function(x) 10^x),
    labels = trans_format("log10", math_format(10^.x))
  ) +
  labs(title = "Coeficientes del modelo Lasso en función de la regularización") +
  theme_bw() +
  theme(legend.position = "none")
```

identificando el número óptimo de componentes del modelo Ridge

```{r}
df_coeficientes_ridge2 %>%
  filter(
    predictor != "(Intercept)",
    coeficiente_ridge2 != 0
) 
```

identificando el número óptimo de componentes del modelo Lasso

```{r}
regularizacion2 %>%
  filter(
    predictor != "(Intercept)",
    coeficientes_lasso2 != 0
) 
```

```{r}
summary(data)
) 
```

\`\`\`
